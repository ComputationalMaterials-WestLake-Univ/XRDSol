[2024-10-11 10:03:51,728][hydra.utils][INFO] - Instantiating <xrdsol.pl_data.datamodule.CrystDataModule>
[2024-10-11 10:03:52,460][numexpr.utils][INFO] - Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
[2024-10-11 10:03:52,460][numexpr.utils][INFO] - NumExpr defaulting to 8 threads.
[2024-10-11 10:04:09,179][hydra.utils][INFO] - Instantiating <xrdsol.pl_modules.diffusion.CSPDiffusion>
[2024-10-11 10:04:10,935][hydra.utils][INFO] - Passing scaler from datamodule to model <StandardScalerTorch(means: -1.219802737236023, stds: 1.0293837785720825)>
[2024-10-11 10:04:10,951][hydra.utils][INFO] - Adding callback <LearningRateMonitor>
[2024-10-11 10:04:10,952][hydra.utils][INFO] - Adding callback <EarlyStopping>
[2024-10-11 10:04:10,953][hydra.utils][INFO] - Adding callback <ModelCheckpoint>
[2024-10-11 10:04:10,954][hydra.utils][INFO] - Instantiating <WandbLogger>
[2024-10-11 10:04:10,956][hydra.utils][INFO] - W&B is now watching <{cfg.logging.wandb_watch.log}>!
[2024-10-11 10:04:10,977][hydra.utils][INFO] - Instantiating the Trainer
[2024-10-11 10:04:11,006][hydra.utils][INFO] - Starting training!
[2024-10-11 10:04:11,009][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2024-10-11 10:04:21,010][torch.distributed.distributed_c10d][INFO] - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=4, worker_count=2, timeout=0:30:00)
[2024-10-11 10:04:24,855][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 4 nodes.
